{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](images/practicum_ai_logo.png) <img src='images/practicumai_deep_learning.png' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "\n",
    "***\n",
    "# *Practicum AI:* Deep Learning - MNIST Classifier\n",
    "\n",
    "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 2.07, page 92).\n",
    "\n",
    "## Amelia's AI Adventure Continues...\n",
    "\n",
    "<img alt=\"A cartoon of Dr. Amelia's dog looking at a computer with a stack of papers next to it showing some handwritten digits.\" src=\"images/Amelias_Dog_MNIST.jpg\" padding=20 align=\"right\" width=250>Amelia and her nutrition studies are back! After her adventures with image recognition and binary classification, she's curious to dive deeper. \n",
    "\n",
    "While Amelia's data collection process is working for most participants in her study, some do not like using the phone application to submit their survey responses. They keep sending in handwritten responses. Realizing that the data from these study participants is still vital to her research, Dr. Amelia is now looking to automate entering these responses using a program to read the numbers that make up the survey responses.\n",
    "\n",
    "Again, Amelia decides to start with the basics: recognizing handwritten numbers. That's where the MNIST dataset comes in. With its vast collection of handwritten digits, it's the perfect training ground for Amelia's next AI venture.\n",
    "\n",
    "**Note:** The cartoon of Dr Amelia's dog was generated with AI's assistance.\n",
    "\n",
    "Training a model on the MNIST dataset is often considered the \"Hello world!\" of AI. It is a commonly used first introduction to image recognition with deep learning.\n",
    "\n",
    "\n",
    "![AI Application Development Pathway model](https://github.com/PracticumAI/deep_learning_2_draft/blob/main/M3-AppDev.00_00_22_23.Still001.png?raw=true)\n",
    "\n",
    " >&#128221; While you're going through this notebook, see if you can figure out which steps here are associated with each of the steps of the Development Pathway.\n",
    "\n",
    "## MNIST Handwritten Digit Classification Dataset\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology) training dataset contains 60,000 28Ã—28 pixel grayscale images of handwritten single digits between 0 and 9, with an additional 10,000 images available for testing. \n",
    "\n",
    "The MNIST dataset is frequently used in machine learning research and has become a standard benchmark for image classification models. Top-performing models often achieve a classification accuracy above 99%, with an error rate between 0.4% and 0.2% on the hold-out test dataset.\n",
    "\n",
    "In this exercise, you will implement a deep neural network (multi-layer) capable of classifying these images of handwritten digits into one of 10 classes. \n",
    "\n",
    "Amelia knows that to start any AI project, she'll need the right tools. She begins by importing the necessary libraries to set the stage for her digit-reading neural network.\n",
    "\n",
    "## 1. Import libraries\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 11:05:50.069037: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-23 11:05:50.108171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 11:05:50.108193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 11:05:50.108950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 11:05:50.114775: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf   # Import the TensorFlow library, which provides tools for machine learning and deep learning.\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting and visualization.\n",
    "# This line allows for the display of plots directly within the Jupyter notebook interface.\n",
    "%matplotlib inline  \n",
    " \n",
    "# Import Keras libraries\n",
    "from tensorflow.keras.models import Sequential  # Import the Sequential model: a linear stack of layers from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Dense  # Import the Dense layer: a fully connected neural network layer from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Flatten  # Import the Flatten layer: used to convert input data into a 1D array from Keras module in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the MNIST dataset\n",
    "\n",
    "Amelia will need to import the MNIST dataset from the [Keras module](https://keras.io/api/datasets/mnist/). The `train_features` and `test_features` variables contain the training and test images, while `train_labels` and `test_labels` contain the corresponding labels for each item in those datasets.  \n",
    "\n",
    "```python\n",
    "# Import the MNIST dataset from TensorFlow's Keras datasets module\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Load the MNIST dataset: \n",
    "# - train_features and train_labels are the training images and their corresponding labels.\n",
    "# - test_features and test_labels are the testing images and their corresponding labels.\n",
    "(train_features,train_labels), (test_features,test_labels) = mnist.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_features,train_labels), (test_features,test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the data\n",
    "\n",
    "Before we start to work with data, it is always good to get a better idea of what we are working with.\n",
    "\n",
    "How many images do we have in our training and testing datasets? (Hint, remember the `len()` function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "len((test_features, test_labels)) + len((train_features, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  19  34  34  34  58 144 144 144 144  96  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 173 253 253 253 253 253 224 253 253 253 244 148  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 211 253 171  97  44  44  25  44  44  92 254 253  44   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   6  10   2   0   0   0   0   0   0 117 254 180   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 124 242 233  27   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4 109 239 253  96   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12 253 253 253  60   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5 166 242 253 254 184 112  41   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  49 162 254 253 253 248 164  51   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  33 110 235 253 253 242 168  44   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10 135 247 254 235 101   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  61 210 253 196   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  26 215 253 167   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 145 253 253   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 117 253 181   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9 199 165  50   0   0   0   0   0   0  87 232 226  45   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11 249 253 244 170  93  12  12  84 160 252 227  92   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  61 180 248 255 253 253 253 253 253 208  39   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 106 187 253 253 213 176 137   6   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   5  33  33  16   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true label for this image is a 3.\n"
     ]
    }
   ],
   "source": [
    "# Set line width for numpy array printing\n",
    "np.set_printoptions(linewidth=150)\n",
    "\n",
    "# Select a random number from train_features\n",
    "select = np.random.randint(0,len(train_features))\n",
    "\n",
    "# Print the image array - longer line length above should allow it to have all 28 rows in 1 line\n",
    "print(train_features[select])\n",
    "\n",
    "# Display the image as an actual image\n",
    "plt.imshow(train_features[select], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Print the true label for the image from train_labels\n",
    "print(f\"The true label for this image is a {train_labels[select]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouptut of the cell above should help clarify how images are encoded in our data. Each pixel has a value from 0 (black) to 255 (white). Since our images are black and white, we only have one grid of pixels. For color images, we would have three: one for each color, red, green, blue.\n",
    "\n",
    "Our datasets have 60,000 images in the `train_features` and 10,000 images in the `test_features`. We will use these data as we move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize the data\n",
    "\n",
    "Before we normalize the data, look to see what the current maximum value is in `train_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it! What is the max value of train_features?\n",
    "train_features.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amelia normalizes the data to ensure her AI model can efficiently process these images. Processing your features so that they are represented by numbers between 0 and 1 is a best practice for AI model development.\n",
    "\n",
    "Normalize the data by scaling the images so their values are between 0 and 1.\n",
    "\n",
    "```python\n",
    "# Normalize the pixel values of the training and testing images to be between 0 and 1.\n",
    "# This is done by dividing each pixel value by 255 (the maximum pixel value for an 8-bit image).\n",
    "# Normalizing improves the training process and convergence.\n",
    "train_features, test_features = train_features / 255.0, test_features / 255.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "train_features, test_features = train_features / 255.0, test_features / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the maximum value of `train features` after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it! After normalization, what is the max value of train_features?\n",
    "train_features.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the sequential model\n",
    "\n",
    "Now, the fun part begins! Amelia sets out to build her neural network. In the previous exercises, Amelia called a pre-trained model for image recognition and then built a single-layer network for her binary classifier. With her confidence high, she is going to create this model herself out of multiple layers. This approach gives her (and you!) the most control over the function of the model.\n",
    "\n",
    "Using the Sequential API, build your model according to the following details:\n",
    "\n",
    "* First, add a flattened layer to unroll the 28x28 pixel images into a single array of 784. The model should use the input_shape in the function argument to set the input size in the first layer.\n",
    "* Add a dense hidden layer with 50 units (neurons) and ReLU (Rectified Linear Unit) activation function.\n",
    "   * The ReLU function will allow the model to capture non-linearities.\n",
    "* Add a second, dense hidden layer with 20 units and ReLU activation function.\n",
    "* Add a dense output layer with 10 units and the softmax activation function.\n",
    "   * We use ten neurons, each representing the digits 0-9. \n",
    "   * The softmax function ensures the output values are probabilities that sum to 1, making it suitable for classification.\n",
    "\n",
    "Here's a graphical view of what we are doing:\n",
    "\n",
    "![A diagram of the neural network being created. It shows the input 28X28 image being flattened into a 784 dimension array. That is the input. There are two hidden, fully connected layers with 50 and 20 neurons each. The final output layer has 10 neurons for the 10 classes in our model.](images/MNIST_neural_network.png)\n",
    "\n",
    "\n",
    "Your completed neural network should have four layers. Feel free to experiment with different architectures and build your own model.\n",
    "\n",
    "```python\n",
    "# Instantiate a Sequential model, which allows us to build a neural network by stacking layers in a linear fashion.\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Flatten layer to the model which transforms a 2D matrix (28x28 pixels) into a 1D array.\n",
    "# This is necessary because our input images are 28x28 pixels, and we need to flatten them to feed into a dense layer.\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Add a Dense (fully connected) layer with 50 units and a ReLU activation function.\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "\n",
    "# Add another Dense layer with 20 units and a ReLU activation function.\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "\n",
    "# Add a final Dense layer with 10 units and a softmax activation function.\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 11:22:51.659443: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2024-07-23 11:22:51.659472: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: c0307a-s25.ufhpc\n",
      "2024-07-23 11:22:51.659477: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: c0307a-s25.ufhpc\n",
      "2024-07-23 11:22:51.659612: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.104.5\n",
      "2024-07-23 11:22:51.659625: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.54.15\n",
      "2024-07-23 11:22:51.659629: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:244] kernel version 550.54.15 does not match DSO version 535.104.5 -- cannot find working devices in this configuration\n"
     ]
    }
   ],
   "source": [
    "# Code it!\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile the model\n",
    "\n",
    "Compiling is the next step. Here, Amelia will specify the parts of the model that are not in the layers, namely the optimizer, loss function, and performance metric.\n",
    "\n",
    "To `compile` the model, you need to specify an optimizer, a loss function, and a metric to judge your model's performance.\n",
    "\n",
    "Compile the model with the following specifications:\n",
    "* `adam` optimizer: An optimization algorithm that adjusts the model weights to minimize the loss.\n",
    "* `sparse_categorical_crossentropy` loss function: Suitable for classification tasks with integer labels.\n",
    "* `accuracy` metric: To monitor the accuracy of the model's predictions during training and evaluation.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect the model configuration using the summary function\n",
    "\n",
    "Display a summary of the model's architecture, including the layers, their shapes, and the number of parameters.\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40480 (158.12 KB)\n",
      "Trainable params: 40480 (158.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Code it!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary indicates that this model has 40,480 parameters (weights and biases). **Note**: If your model summary does not show `Total params: 40480`, double check your model was set up correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fit the model to the training data\n",
    "\n",
    "Now, train the model on the MNIST dataset using the `fit` method. Set the training to run for 10 epochs.\n",
    "\n",
    "Train the model using the training data:\n",
    "* `train_features`: the input images\n",
    "* `train_labels`: the true labels for each image\n",
    "* `epochs=10`: the number of times the model will cycle through the entire dataset\n",
    "\n",
    "```python\n",
    "\n",
    "model.fit(train_features, train_labels, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3309 - accuracy: 0.9038\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1502 - accuracy: 0.9551\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1152 - accuracy: 0.9654\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0949 - accuracy: 0.9711\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0806 - accuracy: 0.9748\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0696 - accuracy: 0.9779\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0607 - accuracy: 0.9809\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0540 - accuracy: 0.9830\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0471 - accuracy: 0.9845\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0421 - accuracy: 0.9860\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0383 - accuracy: 0.9876\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0336 - accuracy: 0.9892\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0310 - accuracy: 0.9900\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0282 - accuracy: 0.9905\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0266 - accuracy: 0.9912\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0240 - accuracy: 0.9922\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0223 - accuracy: 0.9927\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0207 - accuracy: 0.9933\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0190 - accuracy: 0.9938\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0197 - accuracy: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14f6ccd72550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "model.fit(train_features, train_labels, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the model\n",
    "\n",
    "Finally, evaluate your model's performance on the test set by calling the model's `evaluate()` method.\n",
    "\n",
    "\n",
    "Evaluate the model's performance using the testing data:\n",
    "* `test_features`: the input images from the testing set\n",
    "* `test_labels`: the true labels for each image in the testing set\n",
    "\n",
    "The `evaluate` method returns the loss value and any additional metrics (in this case, accuracy) for the model on the testing data.\n",
    "\n",
    "```python\n",
    "model.evaluate(test_features, test_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11952496320009232, 0.9749000072479248]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model predictions\n",
    "\n",
    "Let's see how the model performs on some randomly selected images.  Are its predictions correct?  \n",
    "\n",
    "Randomly select an image from the test dataset, in this case, the 200th image.\n",
    "\n",
    "Select a specific image from the test dataset for examination or prediction.\n",
    "\n",
    "The variable `loc` is set to the index 200, which means we are selecting the 201st image (0-based index) from the test dataset.\n",
    "\n",
    "```python\n",
    "loc = 200\n",
    "\n",
    "# Extract the corresponding image from the test_features array and store it in the 'test_image' variable.\n",
    "test_image = test_features[loc]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "loc = 200\n",
    "test_image = test_features[loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the shape of the image.\n",
    "\n",
    "* Get and display the shape (dimensions) of the `test_image` variable.\n",
    "* This provides insight into the structure and size of the image.\n",
    "\n",
    "```python\n",
    "test_image.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our image is 28x28 pixels. However, the model needs not just the size of the image but also the number of channels. A simple call to the `reshape()` method fixes that problem. \n",
    "\n",
    "* Reshape the 'test_image' from a 2D array (28x28) to a 3D array (1x28x28).\n",
    "* This is commonly done to match the input shape that the model expects when making predictions on single samples.\n",
    "\n",
    "```python\n",
    "test_image = test_image.reshape(1,28,28)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "test_image = test_image.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the model's `predict()` method, assign the output to result, and then view its contents.\n",
    "\n",
    "* Use the trained model to predict the label for the `test_image`.\n",
    "* The `predict` method returns an array of probabilities for each possible label (in the case of MNIST, digits 0-9).\n",
    "* Each value in the array corresponds to the model's predicted probability that the image belongs to a particular class (digit).\n",
    "\n",
    "```python\n",
    "result = model.predict(test_image)\n",
    "\n",
    "# Print the array of probabilities to the console.\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "# Code it!\n",
    "result = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model has returned the probability of 10 predictions, with the highest one being the most likely.  Use the `argmax` function to see the model's prediction.\n",
    "\n",
    "* Use the `argmax` method to find the index (label) of the maximum value in the `result` array.\n",
    "   * This gives us the model's most likely prediction for the class (digit) of the `test_image`.\n",
    "\n",
    "```python\n",
    "result.argmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "result.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the prediction, check the label of the corresponding image.\n",
    "\n",
    "\n",
    "* Using the index loc, retrieve the true label (actual digit) for the `test_image` from the `test_labels` array.\n",
    "   * This gives us the actual class (digit) of the `test_image` to compare with the model's prediction.\n",
    "\n",
    "```python\n",
    "test_labels[loc]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "test_labels[loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualize the image with pyplot.\n",
    "\n",
    "* Use the `imshow` function from the `matplotlib` library to display the `test_image` as a visual image.\n",
    "   * This helps in visually examining the content of the `test_image` (which is represented as a 28x28 array of pixel values).\n",
    "\n",
    "```python\n",
    "plt.imshow(test_features[loc])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14f6767ed9d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code it!\n",
    "plt.imshow(test_features[loc])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And we did it! We helped Amelia create a model that can recognize handwritten digits!\n",
    "\n",
    "\n",
    "## Bonus exercise\n",
    "\n",
    "* Write a function that ties all these steps into one function call. The function should take an input image and print the image with the predicted digit and true digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(image, label):\n",
    "    image.reshape(1, 28, 28)\n",
    "    result = model.predict(test_image)\n",
    "    return f\"Predicted: {result.argmax()}, Actual: {label}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Before continuing\n",
    "###  <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Alert!\n",
    "> Before continuing to another notebook within the same Jupyter session,\n",
    "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**. \n",
    "> This will free up this notebook's GPU memory, making it available for\n",
    "> your next notebook.\n",
    ">\n",
    "> Every time you run multiple notebooks within a Jupyter session with a GPU, this should be done.\n",
    ">\n",
    "> ![Screenshot of the Running Terminals and Kernels tab used t oshut down kernels before starting a new notebook](images/stop_kernel.png)\n",
    "\n",
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Before continuing\n",
    "###  <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Alert!\n",
    "> Before continuing to another notebook within the same Jupyter session,\n",
    "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**. \n",
    "> This will free up this notebook's GPU memory, making it available for\n",
    "> your next notebook.\n",
    ">\n",
    "> Every time you run multiple notebooks within a Jupyter session with a GPU, this should be done.\n",
    ">\n",
    "> ![Screenshot of the Running Terminals and Kernels tab used t oshut down kernels before starting a new notebook](images/stop_kernel.png)\n",
    "\n",
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.15",
   "language": "python",
   "name": "tensorflow-2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
